{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic library\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# 작업 경로 지정\n",
    "CURRENT_FOLDER  = \"your_path\"\n",
    "os.chdir(CURRENT_FOLDER)\n",
    "\n",
    "from src.modeling.metric import * \n",
    "from src.modeling.preprocessing import *   \n",
    "from src.modeling.MLmodel import * \n",
    "\n",
    "with open(os.path.join(\"env.json\")) as f: # input your env file path\n",
    "    envs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1. Model Performance 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE SETTING\n",
    "RESULT_PATH = os.path.join(CURRENT_FOLDER,\"results\",\"general\" )# 수정 using \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "\n",
    "MV_NAME = \"MV\"\n",
    "XGBOOST_NAME = \"XGBOOST\"\n",
    "RF_NAME = \"RF\"\n",
    "DT_NAME = \"DT\"\n",
    "AB_NAME = \"AB\"\n",
    "LDA_NAME = \"LDA\"\n",
    "KNN_NAME = \"KNN\"\n",
    "SVM_NAME = \"SVM\"\n",
    "\n",
    "# METHOD SELECT\n",
    "RANDOM_SEED = 42\n",
    "EXPERIMENT_NUM = 10\n",
    "USE_SMOTE = True\n",
    "\n",
    "# DATA SELECT\n",
    "INCLUDE_PNUM = [1, 2, 3, 7,\n",
    "                12,9,10,\n",
    "                13, 14 , 19, 20, 21, 22, 23] # 15, 16, 17, 4, 8 \n",
    "\n",
    "SENSOR = {\n",
    "    '(S)_call_log': 'in',\n",
    "    \"(NR)_call_log\":'in',\n",
    "    '(NR)_acc': 'in', \n",
    "    '(NR)_fitbit': 'in', \n",
    "    '(S)_env': 'in',\n",
    "    '(S)_customer_audio':'in',\n",
    "    '(S)_customer_utterance':'in',\n",
    "    '(R)_worker_audio' :'in',\n",
    "    '(R)_worker_utterance':'in',\n",
    "    '(S)_customer_transcript':'in',\n",
    "    '(R)_worker_transcript':'in',\n",
    "    \"(IF)_before_work\" : 'in',\n",
    "    \"(IF)_demo\":\"in\"\n",
    "}\n",
    "\n",
    "\n",
    "## LOAD DATA ##\n",
    "audio = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'audio.csv'),parse_dates=['start_second'])\n",
    "call_log = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_log.csv'),parse_dates=['start_second'])\n",
    "fitbit = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'fitbit.csv'), parse_dates=['start_second'])\n",
    "acc = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'acc.csv'),parse_dates=['start_second'])\n",
    "env = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'env.csv'),parse_dates=['start_second'])\n",
    "individual_factor = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'individual_factor.csv'),parse_dates=['date'])\n",
    "transcript = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'transcript.csv'),parse_dates=['start_second'],index_col=False)\n",
    "\n",
    "# label data\n",
    "call_label = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_label.csv'),parse_dates=['start_second'])\n",
    "call_label = call_label.query('pnum in @INCLUDE_PNUM')\n",
    "call_label = remove_unreliable_labels(call_label)\n",
    "\n",
    "## Concatenate ##\n",
    "combined_df = pd.concat([call_log.set_index(['pnum', 'start_second']),\n",
    "                        fitbit.set_index(['pnum', 'start_second']),\n",
    "                        acc.set_index(['pnum', 'start_second']),\n",
    "                        env.set_index(['pnum', 'start_second']),\n",
    "                        audio.set_index(['pnum', 'start_second']),\n",
    "                        transcript.set_index(['pnum', 'start_second'])],\n",
    "                        axis=1, join='inner').reset_index()\n",
    "\n",
    "combined_df['date'] = combined_df['start_second'].dt.date\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df.merge(individual_factor,left_on=['pnum','date'],right_on=['pnum','date'],how='left').drop(['date'],axis=1)\n",
    "final_df = combined_df.merge(call_label,on=['pnum','start_second'],how='inner')\n",
    "\n",
    "# ## Drop missing value ## ==> 모두가 같은 학습 개수를 갖기 위해서\n",
    "final_df = final_df.dropna()\n",
    "\n",
    "## Data selection ##\n",
    "sensor_selected_columns = [key for key, value in SENSOR.items() if value == 'in']\n",
    "final_df = final_df[[col for col in final_df.columns if any(sensor in col for sensor in sensor_selected_columns)] + ['pnum','start_second','surface_acting']]\n",
    "\n",
    "# Distinguishing between feature types\n",
    "all_features = final_df.columns\n",
    "basic_feature = ['pnum','start_second','surface_acting']\n",
    "categorical_feature_ohe = ['(S)_call_log_complaint', '(S)_call_log_weekday',\n",
    "                           '(S)_call_log_hour_category', '(IF)_demo_engage_motivation']\n",
    "categorical_feature_no_ohe = ['(IF)_demo_age_category', '(IF)_demo_career_category']\n",
    "categorical_feature_binary = ['(IF)_demo_gender', '(IF)_demo_education']\n",
    "categorical_features = {\n",
    "    \"categorical_feature_ohe\": [feature for feature in categorical_feature_ohe if any(key in feature for key in SENSOR if SENSOR[key] == 'in')],\n",
    "    \"categorical_feature_no_ohe\": [feature for feature in categorical_feature_no_ohe if any(key in feature for key in SENSOR if SENSOR[key] == 'in')],\n",
    "    \"categorical_feature_binary\": [feature for feature in categorical_feature_binary if any(key in feature for key in SENSOR if SENSOR[key] == 'in')]\n",
    "}\n",
    "numeric_feature = [feature for feature in all_features if feature not in basic_feature + categorical_features['categorical_feature_ohe'] + categorical_features['categorical_feature_no_ohe'] + categorical_features['categorical_feature_binary']]\n",
    "\n",
    "## One-hot-encoding\n",
    "if len(categorical_features['categorical_feature_ohe'])>0:\n",
    "    final_df[categorical_features['categorical_feature_ohe']] = final_df[categorical_features['categorical_feature_ohe']].astype('int')\n",
    "    # one-hot encodingDS\n",
    "    final_df = one_hot(final_df,categorical_features['categorical_feature_ohe'])\n",
    "\n",
    "\n",
    "# Model \n",
    "result_xgb_mean_df, x_feature_df = XGBoost_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_xgb_mean_df.to_csv(os.path.join(RESULT_PATH,XGBOOST_NAME+'.csv'))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,XGBOOST_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_rf_mean_df,  x_feature_df = RandomForest_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_rf_mean_df.to_csv(os.path.join(RESULT_PATH,RF_NAME+'.csv'))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,RF_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_lda_mean_df, x_feature_df = LDA_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_lda_mean_df.to_csv(os.path.join(RESULT_PATH,LDA_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,LDA_NAME+\"+_x_feature.csv\"))\n",
    "\n",
    "result_svm_mean_df, x_feature_df = SVM_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_svm_mean_df.to_csv(os.path.join(RESULT_PATH,SVM_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,SVM_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_knn_mean_df, x_feature_df = KNN_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_knn_mean_df.to_csv(os.path.join(RESULT_PATH,KNN_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,KNN_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_ab_mean_df, x_feature_df = AdaBoost_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_ab_mean_df.to_csv(os.path.join(RESULT_PATH,AB_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,AB_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_dt_mean_df,  x_feature_df = DecisionTree_General(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_dt_mean_df.to_csv(os.path.join(RESULT_PATH,DT_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,DT_NAME+\"_x_feature.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE SETTING\n",
    "RESULT_PATH = os.path.join(CURRENT_FOLDER,\"results\",\"personalization\" )# 수정 using \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "\n",
    "MV_NAME = \"MV_personalized\"\n",
    "XGBOOST_NAME = \"XGBOOST_personalized\"\n",
    "RF_NAME = \"RF_personalized\"\n",
    "DT_NAME = \"DT_personalized\"\n",
    "AB_NAME = \"AB_personalized\"\n",
    "LDA_NAME = \"LDA_personalized\"\n",
    "KNN_NAME = \"KNN_personalized\"\n",
    "SVM_NAME = \"SVM_personalized\"\n",
    "\n",
    "# METHOD SELECT\n",
    "RANDOM_SEED = 42\n",
    "EXPERIMENT_NUM = 1\n",
    "USE_SMOTE = True\n",
    "\n",
    "# DATA SELECT\n",
    "INCLUDE_PNUM = [1, 2, 3, 7,\n",
    "                12,9,10,\n",
    "                13, 14 , 19, 20, 21, 22, 23] # 15, 16, 17, 4, 8 \n",
    "\n",
    "SENSOR = {\n",
    "    '(S)_call_log': 'out',\n",
    "    \"(NR)_call_log\":'out',\n",
    "    '(NR)_acc': 'in', \n",
    "    '(NR)_fitbit': 'in', \n",
    "    '(S)_env': 'out',\n",
    "    '(S)_customer_audio':'out',\n",
    "    '(S)_customer_utterance':'out',\n",
    "    '(R)_worker_audio' :'out',\n",
    "    '(R)_worker_utterance':'out',\n",
    "    '(S)_customer_transcript':'out',\n",
    "    '(R)_worker_transcript':'out',\n",
    "    \"(IF)_before_work\" : 'out',\n",
    "    \"(IF)_demo\":'out'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "## LOAD DATA ##\n",
    "audio = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'audio.csv'),parse_dates=['start_second'])\n",
    "call_log = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_log.csv'),parse_dates=['start_second'])\n",
    "fitbit = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'fitbit.csv'), parse_dates=['start_second'])\n",
    "acc = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'acc.csv'),parse_dates=['start_second'])\n",
    "env = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'env.csv'),parse_dates=['start_second'])\n",
    "individual_factor = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'individual_factor.csv'),parse_dates=['date'])\n",
    "transcript = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'transcript.csv'),parse_dates=['start_second'],index_col=False)\n",
    "\n",
    "# label data\n",
    "call_label = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_label.csv'),parse_dates=['start_second'])\n",
    "call_label = call_label.query('pnum in @INCLUDE_PNUM')\n",
    "call_label = remove_unreliable_labels(call_label)\n",
    "\n",
    "## Concatenate ##\n",
    "combined_df = pd.concat([call_log.set_index(['pnum', 'start_second']),\n",
    "                        fitbit.set_index(['pnum', 'start_second']),\n",
    "                        acc.set_index(['pnum', 'start_second']),\n",
    "                        env.set_index(['pnum', 'start_second']),\n",
    "                        audio.set_index(['pnum', 'start_second']),\n",
    "                        transcript.set_index(['pnum', 'start_second'])],\n",
    "                        axis=1, join='inner').reset_index()\n",
    "combined_df['date'] = combined_df['start_second'].dt.date\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df.merge(individual_factor,left_on=['pnum','date'],right_on=['pnum','date'],how='left').drop(['date'],axis=1)\n",
    "final_df = combined_df.merge(call_label,on=['pnum','start_second'],how='inner')\n",
    "\n",
    "## Drop missing value ## ==> 모두가 같은 학습 개수를 갖기 위해서\n",
    "final_df = final_df.dropna()\n",
    "\n",
    "## Data selection ##\n",
    "sensor_selected_columns = [key for key, value in SENSOR.items() if value == 'in']\n",
    "final_df = final_df[[col for col in final_df.columns if any(sensor in col for sensor in sensor_selected_columns)] + ['pnum','start_second','surface_acting']]\n",
    "\n",
    "# Distinguishing between feature types\n",
    "all_features = final_df.columns\n",
    "basic_feature = ['pnum','start_second','surface_acting']\n",
    "categorical_feature_ohe = ['(S)_call_log_complaint', '(S)_call_log_weekday',\n",
    "                           '(S)_call_log_hour_category', '(IF)_demo_engage_motivation']\n",
    "categorical_feature_no_ohe = ['(IF)_demo_age_category', '(IF)_demo_career_category']\n",
    "categorical_feature_binary = ['(IF)_demo_gender', '(IF)_demo_education']\n",
    "categorical_features = {\n",
    "    \"categorical_feature_ohe\": [feature for feature in categorical_feature_ohe if any(key in feature for key in SENSOR if SENSOR[key] == 'in')],\n",
    "    \"categorical_feature_no_ohe\": [feature for feature in categorical_feature_no_ohe if any(key in feature for key in SENSOR if SENSOR[key] == 'in')],\n",
    "    \"categorical_feature_binary\": [feature for feature in categorical_feature_binary if any(key in feature for key in SENSOR if SENSOR[key] == 'in')]\n",
    "}\n",
    "numeric_feature = [feature for feature in all_features if feature not in basic_feature + categorical_features['categorical_feature_ohe'] + categorical_features['categorical_feature_no_ohe'] + categorical_features['categorical_feature_binary']]\n",
    "\n",
    "## One-hot-encoding\n",
    "if len(categorical_features['categorical_feature_ohe'])>0:\n",
    "    final_df[categorical_features['categorical_feature_ohe']] = final_df[categorical_features['categorical_feature_ohe']].astype('int')\n",
    "    # one-hot encoding\n",
    "    final_df = one_hot(final_df,categorical_features['categorical_feature_ohe'])\n",
    "\n",
    "result_xgb_mean_df, x_feature_df = XGBoost_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_xgb_mean_df.to_csv(os.path.join(RESULT_PATH,XGBOOST_NAME+'.csv'))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,XGBOOST_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_rf_mean_df,  x_feature_df = RandomForest_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_rf_mean_df.to_csv(os.path.join(RESULT_PATH,RF_NAME+'.csv'))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,RF_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_lda_mean_df, x_feature_df = LDA_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_lda_mean_df.to_csv(os.path.join(RESULT_PATH,LDA_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,LDA_NAME+\"+_x_feature.csv\"))\n",
    "\n",
    "result_svm_mean_df, x_feature_df = SVM_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_svm_mean_df.to_csv(os.path.join(RESULT_PATH,SVM_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,SVM_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_knn_mean_df, x_feature_df = KNN_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_knn_mean_df.to_csv(os.path.join(RESULT_PATH,KNN_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,KNN_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_ab_mean_df, x_feature_df = AdaBoost_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_ab_mean_df.to_csv(os.path.join(RESULT_PATH,AB_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,AB_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_dt_mean_df,  x_feature_df = DecisionTree_Personalized(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_dt_mean_df.to_csv(os.path.join(RESULT_PATH,DT_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,DT_NAME+\"_x_feature.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE SETTING\n",
    "RESULT_PATH = os.path.join(CURRENT_FOLDER,\"results\",\"hybrid\" )# 수정 using \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "\n",
    "MV_NAME = \"MV_hybrid\"\n",
    "XGBOOST_NAME = \"XGBOOST_hybrid\"\n",
    "RF_NAME = \"RF_hybrid\"\n",
    "DT_NAME = \"DT_hybrid\"\n",
    "AB_NAME = \"AB_hybrid\"\n",
    "LDA_NAME = \"LDA_hybrid\"\n",
    "KNN_NAME = \"KNN_hybrid\"\n",
    "SVM_NAME = \"SVM_hybrid\"\n",
    "\n",
    "# METHOD SELECT\n",
    "RANDOM_SEED = 42\n",
    "EXPERIMENT_NUM = 10\n",
    "USE_SMOTE = True\n",
    "\n",
    "# DATA SELECT\n",
    "INCLUDE_PNUM = [1, 2, 3, 7,\n",
    "                12,9,10,\n",
    "                13, 14 , 19, 20, 21, 22, 23] # 15, 16, 17, 4, 8 \n",
    "\n",
    "\n",
    "SENSOR = {\n",
    "    '(S)_call_log': 'in',\n",
    "    \"(NR)_call_log\":'in',\n",
    "    '(NR)_acc': 'in', \n",
    "    '(NR)_fitbit': 'in', \n",
    "    '(S)_env': 'in',\n",
    "    '(S)_customer_audio':'in',\n",
    "    '(S)_customer_utterance':'in',\n",
    "    '(R)_worker_audio' :'in',\n",
    "    '(R)_worker_utterance':'in',\n",
    "    '(S)_customer_transcript':'in',\n",
    "    '(R)_worker_transcript':'in',\n",
    "    \"(IF)_before_work\" : 'in',\n",
    "    \"(IF)_demo\":\"in\"\n",
    "}\n",
    "\n",
    "\n",
    "## LOAD DATA ##\n",
    "audio = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'audio.csv'),parse_dates=['start_second'])\n",
    "call_log = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_log.csv'),parse_dates=['start_second'])\n",
    "fitbit = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'fitbit.csv'), parse_dates=['start_second'])\n",
    "acc = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'acc.csv'),parse_dates=['start_second'])\n",
    "env = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'env.csv'),parse_dates=['start_second'])\n",
    "individual_factor = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'individual_factor.csv'),parse_dates=['date'])\n",
    "transcript = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'transcript.csv'),parse_dates=['start_second'],index_col=False)\n",
    "\n",
    "# label data\n",
    "call_label = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_label.csv'),parse_dates=['start_second'])\n",
    "call_label = call_label.query('pnum in @INCLUDE_PNUM')\n",
    "call_label = remove_unreliable_labels(call_label)\n",
    "\n",
    "## Concatenate ##\n",
    "combined_df = pd.concat([call_log.set_index(['pnum', 'start_second']),\n",
    "                        fitbit.set_index(['pnum', 'start_second']),\n",
    "                        acc.set_index(['pnum', 'start_second']),\n",
    "                        env.set_index(['pnum', 'start_second']),\n",
    "                        audio.set_index(['pnum', 'start_second']),\n",
    "                        transcript.set_index(['pnum', 'start_second'])],\n",
    "                        axis=1, join='inner').reset_index()\n",
    "\n",
    "combined_df['date'] = combined_df['start_second'].dt.date\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "combined_df = combined_df.merge(individual_factor,left_on=['pnum','date'],right_on=['pnum','date'],how='left').drop(['date'],axis=1)\n",
    "final_df = combined_df.merge(call_label,on=['pnum','start_second'],how='inner')\n",
    "\n",
    "# ## Drop missing value ## ==> 모두가 같은 학습 개수를 갖기 위해서\n",
    "final_df = final_df.dropna()\n",
    "\n",
    "\n",
    "## Data selection ##\n",
    "sensor_selected_columns = [key for key, value in SENSOR.items() if value == 'in']\n",
    "final_df = final_df[[col for col in final_df.columns if any(sensor in col for sensor in sensor_selected_columns)] + ['pnum','start_second','surface_acting']]\n",
    "\n",
    "# Distinguishing between feature types\n",
    "all_features = final_df.columns\n",
    "basic_feature = ['pnum','start_second','surface_acting']\n",
    "categorical_feature_ohe = ['(S)_call_log_complaint', '(S)_call_log_weekday',\n",
    "                           '(S)_call_log_hour_category','(IF)_demo_engage_motivation'] # '(IF)_demo_engage_motivation']\n",
    "categorical_feature_no_ohe = ['(IF)_demo_age_category', '(IF)_demo_career_category']\n",
    "categorical_feature_binary = ['(IF)_demo_gender', '(IF)_demo_education']\n",
    "categorical_features = {\n",
    "    \"categorical_feature_ohe\": [feature for feature in categorical_feature_ohe if any(key in feature for key in SENSOR if SENSOR[key] == 'in')],\n",
    "    \"categorical_feature_no_ohe\": [feature for feature in categorical_feature_no_ohe if any(key in feature for key in SENSOR if SENSOR[key] == 'in')],\n",
    "    \"categorical_feature_binary\": [feature for feature in categorical_feature_binary if any(key in feature for key in SENSOR if SENSOR[key] == 'in')]\n",
    "}\n",
    "numeric_feature = [feature for feature in all_features if feature not in basic_feature + categorical_features['categorical_feature_ohe'] + categorical_features['categorical_feature_no_ohe'] + categorical_features['categorical_feature_binary']]\n",
    "\n",
    "## One-hot-encoding\n",
    "if len(categorical_features['categorical_feature_ohe'])>0:\n",
    "    final_df[categorical_features['categorical_feature_ohe']] = final_df[categorical_features['categorical_feature_ohe']].astype('int')\n",
    "    # one-hot encodingDS\n",
    "    final_df = one_hot(final_df,categorical_features['categorical_feature_ohe'])\n",
    "\n",
    "result_xgb_mean_df, x_feature_df = XGBoost_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_xgb_mean_df.to_csv(os.path.join(RESULT_PATH,XGBOOST_NAME+'.csv'))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,XGBOOST_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_rf_mean_df,  x_feature_df = RandomForest_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_rf_mean_df.to_csv(os.path.join(RESULT_PATH,RF_NAME+'.csv'))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,RF_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_lda_mean_df, x_feature_df = LDA_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_lda_mean_df.to_csv(os.path.join(RESULT_PATH,LDA_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,LDA_NAME+\"+_x_feature.csv\"))\n",
    "\n",
    "result_svm_mean_df, x_feature_df = SVM_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_svm_mean_df.to_csv(os.path.join(RESULT_PATH,SVM_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,SVM_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_knn_mean_df, x_feature_df = KNN_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_knn_mean_df.to_csv(os.path.join(RESULT_PATH,KNN_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,KNN_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_ab_mean_df, x_feature_df = AdaBoost_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_ab_mean_df.to_csv(os.path.join(RESULT_PATH,AB_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,AB_NAME+\"_x_feature.csv\"))\n",
    "\n",
    "result_dt_mean_df,  x_feature_df = DecisionTree_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "result_dt_mean_df.to_csv(os.path.join(RESULT_PATH,DT_NAME))\n",
    "x_feature_df.to_csv(os.path.join(RESULT_PATH,DT_NAME+\"_x_feature.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2. Ablation Study - ER Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = os.path.join(CURRENT_FOLDER,\"results\",\"ER_ablation_RF\" )# 수정 using \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "    \n",
    "# METHOD SELECT\n",
    "RANDOM_SEED = 42\n",
    "EXPERIMENT_NUM = 5\n",
    "USE_SMOTE = True\n",
    "\n",
    "# DATA SELECT\n",
    "INCLUDE_PNUM = [1, 2, 3, 7,\n",
    "                12,9,10,\n",
    "                13, 14 , 19, 20, 21, 22, 23] # 15, 16, 17, 4, 8 \n",
    "\n",
    "def ER_ablation_study(save_name, sensor_dic):\n",
    "    ## LOAD DATA ##\n",
    "    audio = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'audio.csv'),parse_dates=['start_second'])\n",
    "    call_log = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_log.csv'),parse_dates=['start_second'])\n",
    "    fitbit = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'fitbit.csv'), parse_dates=['start_second'])\n",
    "    acc = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'acc.csv'),parse_dates=['start_second'])\n",
    "    env = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'env.csv'),parse_dates=['start_second'])\n",
    "    individual_factor = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'individual_factor.csv'),parse_dates=['date'])\n",
    "    transcript = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'transcript.csv'),parse_dates=['start_second'],index_col=False)\n",
    "\n",
    "    # label data\n",
    "    call_label = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_label.csv'),parse_dates=['start_second'])\n",
    "    call_label = call_label.query('pnum in @INCLUDE_PNUM')\n",
    "    call_label = remove_unreliable_labels(call_label)\n",
    "\n",
    "    ## Concatenate ##\n",
    "    combined_df = pd.concat([call_log.set_index(['pnum', 'start_second']),\n",
    "                            fitbit.set_index(['pnum', 'start_second']),\n",
    "                            acc.set_index(['pnum', 'start_second']),\n",
    "                            env.set_index(['pnum', 'start_second']),\n",
    "                            audio.set_index(['pnum', 'start_second']),\n",
    "                            transcript.set_index(['pnum', 'start_second'])],\n",
    "                            axis=1, join='inner').reset_index()\n",
    "    combined_df['date'] = combined_df['start_second'].dt.date\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "    combined_df = combined_df.merge(individual_factor,left_on=['pnum','date'],right_on=['pnum','date'],how='left').drop(['date'],axis=1)\n",
    "    final_df = combined_df.merge(call_label,on=['pnum','start_second'],how='inner')\n",
    "\n",
    "    ## Drop missing value ## ==> 모두가 같은 학습 개수를 갖기 위해서\n",
    "    final_df = final_df.dropna()\n",
    "\n",
    "    ## Data selection ##\n",
    "    sensor_selected_columns = [key for key, value in sensor_dic.items() if value == 'in']\n",
    "    final_df = final_df[[col for col in final_df.columns if any(sensor in col for sensor in sensor_selected_columns)] + ['pnum','start_second','surface_acting']]\n",
    "\n",
    "    # Distinguishing between feature types\n",
    "    all_features = final_df.columns\n",
    "    basic_feature = ['pnum','start_second','surface_acting']\n",
    "    categorical_feature_ohe = ['(S)_call_log_complaint', '(S)_call_log_weekday',\n",
    "                            '(S)_call_log_hour_category', '(IF)_demo_engage_motivation']\n",
    "    categorical_feature_no_ohe = ['(IF)_demo_age_category', '(IF)_demo_career_category']\n",
    "    categorical_feature_binary = ['(IF)_demo_gender', '(IF)_demo_education']\n",
    "    categorical_features = {\n",
    "        \"categorical_feature_ohe\": [feature for feature in categorical_feature_ohe if any(key in feature for key in sensor_dic if sensor_dic[key] == 'in')],\n",
    "        \"categorical_feature_no_ohe\": [feature for feature in categorical_feature_no_ohe if any(key in feature for key in sensor_dic if sensor_dic[key] == 'in')],\n",
    "        \"categorical_feature_binary\": [feature for feature in categorical_feature_binary if any(key in feature for key in sensor_dic if sensor_dic[key] == 'in')]\n",
    "    }\n",
    "    numeric_feature = [feature for feature in all_features if feature not in basic_feature + categorical_features['categorical_feature_ohe'] + categorical_features['categorical_feature_no_ohe'] + categorical_features['categorical_feature_binary']]\n",
    "\n",
    "    ## One-hot-encoding\n",
    "    if len(categorical_features['categorical_feature_ohe'])>0:\n",
    "        final_df[categorical_features['categorical_feature_ohe']] = final_df[categorical_features['categorical_feature_ohe']].astype('int')\n",
    "        # one-hot encoding\n",
    "        final_df = one_hot(final_df,categorical_features['categorical_feature_ohe'])\n",
    "\n",
    "\n",
    "    result_rf_mean_df,  x_feature_df = RandomForest_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "    result_rf_mean_df.to_csv(os.path.join(RESULT_PATH,save_name+'.csv'))\n",
    "    x_feature_df.to_csv(os.path.join(RESULT_PATH,save_name+\"_x_feature.csv\"))\n",
    "    # result_xgb_mean_df, x_feature_df = XGBoost_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "    # result_xgb_mean_df.to_csv(os.path.join(RESULT_PATH,save_name+'.csv'))\n",
    "    # x_feature_df.to_csv(os.path.join(RESULT_PATH,save_name+\"_x_feature.csv\"))\n",
    "    \n",
    "\n",
    "\n",
    "ER_ablation =[\n",
    "    [\"(S)\"],[\"(R)\"],[\"(NR)\"], [\"(IF)\"],\n",
    "              [\"(S)\",\"(R)\"],[\"(S)\",\"(NR)\"],[\"(S)\",\"(IF)\"],\n",
    "              [\"(S)\",\"(R)\",\"(NR)\"],[\"(S)\",\"(R)\",\"(IF)\"],\n",
    "              [\"(R)\",\"(NR)\"],[\"(R)\",\"(NR)\",\"(IF)\"],\n",
    "              [\"(S)\",\"(R)\",\"(NR)\",\"(IF)\"]\n",
    "              ]\n",
    "\n",
    "\n",
    "SENSOR = {\n",
    "    '(S)_call_log': 'out',\n",
    "    \"(NR)_call_log\":'out',\n",
    "    '(NR)_acc': 'out', \n",
    "    '(NR)_fitbit': 'out', \n",
    "    '(S)_env': 'out',\n",
    "    '(S)_customer_audio':'out',\n",
    "    '(S)_customer_utterance':'out',\n",
    "    '(R)_worker_audio' :'out',\n",
    "    '(R)_worker_utterance':'out',\n",
    "    '(S)_customer_transcript':'out',\n",
    "    '(R)_worker_transcript':'in',\n",
    "    \"(IF)_before_work\" : 'out',\n",
    "    \"(IF)_demo\":\"out\"\n",
    "}\n",
    "\n",
    "# 조합에 따라 \"in\" 및 \"out\"을 설정하고 새로운 sensor_dic과 이름을 반환하는 함수\n",
    "def generate_ablation_sensor_dic(ablation_list, sensor_dict):\n",
    "    results = []\n",
    "    \n",
    "    for ablation in ablation_list:\n",
    "        # 새로운 sensor_dic을 생성하고 \"in\"과 \"out\"을 설정\n",
    "        new_sensor_dic = {}\n",
    "        for key in sensor_dict.keys():\n",
    "            if any(marker in key for marker in ablation):\n",
    "                new_sensor_dic[key] = 'in'\n",
    "            else:\n",
    "                new_sensor_dic[key] = 'out'\n",
    "        \n",
    "        # 이름 만들기\n",
    "        config_name = \"_\".join(ablation)\n",
    "        \n",
    "        # 결과 저장\n",
    "        results.append((config_name, new_sensor_dic))\n",
    "    \n",
    "    return results\n",
    "\n",
    "ablation_sensor_configs = generate_ablation_sensor_dic(ER_ablation, SENSOR)\n",
    "for name, sensor_dic in ablation_sensor_configs:\n",
    "    ER_ablation_study(name, sensor_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ2. Ablation Study - Data modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = os.path.join(CURRENT_FOLDER,\"results\",\"Data_ablation_more\" )# 수정 using \n",
    "if not os.path.exists(RESULT_PATH):\n",
    "    os.makedirs(RESULT_PATH)\n",
    "    \n",
    "# METHOD SELECT\n",
    "RANDOM_SEED = 42\n",
    "EXPERIMENT_NUM = 1\n",
    "USE_SMOTE = True\n",
    "\n",
    "# DATA SELECT\n",
    "INCLUDE_PNUM = [1, 2, 3, 7,\n",
    "                12,9,10,\n",
    "                13, 14 , 19, 20, 21, 22, 23] # 15, 16, 17, 4, 8 \n",
    "\n",
    "def Data_ablation_study(save_name, sensor_dic):\n",
    "    ## LOAD DATA ##\n",
    "    audio = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'audio.csv'),parse_dates=['start_second'])\n",
    "    call_log = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_log.csv'),parse_dates=['start_second'])[['pnum','start_second','(S)_call_log_duration','(S)_call_log_complaint']]\n",
    "    fitbit = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'fitbit.csv'), parse_dates=['start_second'])\n",
    "    acc = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'acc.csv'),parse_dates=['start_second'])\n",
    "    env = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'env.csv'),parse_dates=['start_second'])\n",
    "    individual_factor = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'individual_factor.csv'),parse_dates=['date'])\n",
    "    transcript = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'transcript.csv'),parse_dates=['start_second'],index_col=False)\n",
    "\n",
    "    # label data\n",
    "    call_label = pd.read_csv(os.path.join(envs['DATA_PATH'], '3_feature_extraction', 'call_label.csv'),parse_dates=['start_second'])\n",
    "    call_label = call_label.query('pnum in @INCLUDE_PNUM')\n",
    "    call_label = remove_unreliable_labels(call_label)\n",
    "\n",
    "    ## Concatenate ##\n",
    "    combined_df = pd.concat([call_log.set_index(['pnum', 'start_second']),\n",
    "                            fitbit.set_index(['pnum', 'start_second']),\n",
    "                            acc.set_index(['pnum', 'start_second']),\n",
    "                            env.set_index(['pnum', 'start_second']),\n",
    "                            audio.set_index(['pnum', 'start_second']),\n",
    "                            transcript.set_index(['pnum', 'start_second'])],\n",
    "                            axis=1, join='inner').reset_index()\n",
    "    combined_df['date'] = combined_df['start_second'].dt.date\n",
    "    combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "    combined_df = combined_df.merge(individual_factor,left_on=['pnum','date'],right_on=['pnum','date'],how='left').drop(['date'],axis=1)\n",
    "    final_df = combined_df.merge(call_label,on=['pnum','start_second'],how='inner')\n",
    "\n",
    "    ## Drop missing value ## ==> 모두가 같은 학습 개수를 갖기 위해서\n",
    "    final_df = final_df.dropna()\n",
    "\n",
    "    ## Data selection ##\n",
    "    sensor_selected_columns = [key for key, value in sensor_dic.items() if value == 'in']\n",
    "    final_df = final_df[[col for col in final_df.columns if any(sensor in col for sensor in sensor_selected_columns)] + ['pnum','start_second','surface_acting']]\n",
    "    print(\"최종 생성된 데이터: \",final_df.columns)\n",
    "\n",
    "    # Distinguishing between feature types\n",
    "    all_features = final_df.columns\n",
    "    basic_feature = ['pnum','start_second','surface_acting']\n",
    "    categorical_feature_ohe = ['(S)_call_log_complaint', '(S)_call_log_weekday',\n",
    "                            '(S)_call_log_hour_category', '(IF)_demo_engage_motivation']\n",
    "    categorical_feature_no_ohe = ['(IF)_demo_age_category', '(IF)_demo_career_category']\n",
    "    categorical_feature_binary = ['(IF)_demo_gender', '(IF)_demo_education']\n",
    "    categorical_features = {\n",
    "        \"categorical_feature_ohe\": [feature for feature in categorical_feature_ohe if any(key in feature for key in sensor_dic if sensor_dic[key] == 'in')],\n",
    "        \"categorical_feature_no_ohe\": [feature for feature in categorical_feature_no_ohe if any(key in feature for key in sensor_dic if sensor_dic[key] == 'in')],\n",
    "        \"categorical_feature_binary\": [feature for feature in categorical_feature_binary if any(key in feature for key in sensor_dic if sensor_dic[key] == 'in')]\n",
    "    }\n",
    "    numeric_feature = [feature for feature in all_features if feature not in basic_feature + categorical_features['categorical_feature_ohe'] + categorical_features['categorical_feature_no_ohe'] + categorical_features['categorical_feature_binary']]\n",
    "\n",
    "    # ## One-hot-encoding\n",
    "    # if len(categorical_features['categorical_feature_ohe'])>0:\n",
    "    #     final_df[categorical_features['categorical_feature_ohe']] = final_df[categorical_features['categorical_feature_ohe']].astype('int')\n",
    "    #     # one-hot encoding\n",
    "    #     final_df = one_hot(final_df,categorical_features['categorical_feature_ohe'])\n",
    "    \n",
    "    result_rf_mean_df,  x_feature_df = RandomForest_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "    result_rf_mean_df.to_csv(os.path.join(RESULT_PATH,save_name+'.csv'))\n",
    "    x_feature_df.to_csv(os.path.join(RESULT_PATH,save_name+\"_x_feature.csv\"))\n",
    "\n",
    "    # result_xgb_mean_df, x_feature_df = XGBoost_Hybrid(final_df,numeric_feature,EXPERIMENT_NUM,RANDOM_SEED,USE_SMOTE, RESULT_PATH)\n",
    "    # result_xgb_mean_df.to_csv(os.path.join(RESULT_PATH,save_name+'.csv'))\n",
    "    # x_feature_df.to_csv(os.path.join(RESULT_PATH,save_name+\"_x_feature.csv\"))\n",
    "    \n",
    "\n",
    "\n",
    "SENSOR = {\n",
    "    '(S)_call_log': 'out',\n",
    "    \"(NR)_call_log\":'out',\n",
    "    '(NR)_acc': 'out', \n",
    "    '(NR)_fitbit': 'out', \n",
    "    '(S)_env': 'out',\n",
    "    '(S)_customer_audio':'out',\n",
    "    '(S)_customer_utterance':'out',\n",
    "    '(R)_worker_audio' :'out',\n",
    "    '(R)_worker_utterance':'out',\n",
    "    '(S)_customer_transcript':'out',\n",
    "    '(R)_worker_transcript':'in',\n",
    "    \"(IF)_before_work\" : 'out',\n",
    "    \"(IF)_demo\":\"out\"\n",
    "}\n",
    "\n",
    "Data_ablation =[\n",
    "    # ['(R)_worker_transcript','(R)_worker_utterance','(R)_worker_audio'],\n",
    "    ['(S)_customer_audio','(S)_customer_transcript'] # '(S)_call_log', '(S)_customer_utterance'\n",
    "    # ['(R)_worker_utterance','(R)_worker_audio']/\n",
    "    # [\"(S)_customer_audio\",\"(S)_customer_transcript\"],['(R)_worker_transcript',\"(R)_worker_utterance\"]\n",
    "    # [\"(S)_customer_audio\",\"(S)_customer_transcript\",\"(S)_customer_utterance\"],\n",
    "    # [\"(S)_customer_audio\",\"(S)_customer_transcript\",\"(S)_customer_utterance\",'(S)_call_log']\n",
    "    # [\"(S)_customer_audio\"],[\"(S)_customer_transcript\"],[\"(S)_customer_utterance\"],[\"(S)_env\"],['(S)_call_log'],['(R)_worker_audio'],['(R)_worker_transcript'],[\"(R)_worker_utterance\"],['(NR)_fitbit'],[\"(NR)_call_log\",'(NR)_acc'],\n",
    "                # [\"(IF)_before_work\"],\n",
    "                # [\"(IF)_demo\"]\n",
    "                ] # \"(S)_call_log\"는 따로 해야 함\n",
    "\n",
    "# # 조합에 따라 \"in\" 및 \"out\"을 설정하고 새로운 sensor_dic과 이름을 반환하는 함수\n",
    "def generate_ablation_sensor_dic(ablation_list, sensor_dict):\n",
    "    results = []\n",
    "    \n",
    "    for ablation in ablation_list:\n",
    "        # 새로운 sensor_dic을 생성하고 \"in\"과 \"out\"을 설정\n",
    "        new_sensor_dic = {}\n",
    "        for key in sensor_dict.keys():\n",
    "            if any(marker in key for marker in ablation):\n",
    "                new_sensor_dic[key] = 'in'\n",
    "            else:\n",
    "                new_sensor_dic[key] = 'out'\n",
    "        \n",
    "        # 이름 만들기\n",
    "        config_name = \"_\".join(ablation)\n",
    "        \n",
    "        # 결과 저장\n",
    "        results.append((config_name, new_sensor_dic))\n",
    "    \n",
    "    return results\n",
    "\n",
    "ablation_sensor_configs = generate_ablation_sensor_dic(Data_ablation, SENSOR)\n",
    "for name, sensor_dic in ablation_sensor_configs:\n",
    "    Data_ablation_study(name, sensor_dic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
