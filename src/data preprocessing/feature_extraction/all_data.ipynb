{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import metrics # for evaluations\n",
    "from sklearn.cluster import KMeans \n",
    "import statistics\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "with open(\"env.json\") as f: # inpuy your env file path\n",
    "    envs = json.load(f)\n",
    "\n",
    "# 폰트 경로 지정\n",
    "font_path = envs['FONT_PATH']\n",
    "prop = FontProperties(fname=font_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function \n",
    "def attatch_prefix_condition(df, prefix,exclude_col = ['pnum','start_second','end','date','matching']):\n",
    "    df.columns = [f\"{prefix}_{col}\" if col not in exclude_col else col for col in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signal_sum(sampled_signal):\n",
    "    return np.sum(sampled_signal)\n",
    "\n",
    "def signal_mean(sampled_signal):\n",
    "\n",
    "    return np.mean(sampled_signal)\n",
    "\n",
    "def signal_min(sampled_signal):\n",
    "    return np.min(sampled_signal)\n",
    "\n",
    "def signal_max(sampled_signal):\n",
    "    return np.max(sampled_signal)\n",
    "\n",
    "def signal_q1(sampled_signal):\n",
    "    return np.percentile(sampled_signal, 25)\n",
    "\n",
    "def signal_q2(sampled_signal):\n",
    "    return np.percentile(sampled_signal, 50)\n",
    "\n",
    "def signal_q3(sampled_signal):\n",
    "    return np.percentile(sampled_signal, 75)\n",
    "\n",
    "def signal_20th(sampled_signal):\n",
    "    return np.percentile(sampled_signal, 20)\n",
    "\n",
    "def signal_80th(sampled_signal):\n",
    "    return np.percentile(sampled_signal, 80)\n",
    "\n",
    "def signal_standard_deviation(sampled_signal): # 10초에 한번 씩 수집되기 때문에\n",
    "    try:\n",
    "        return statistics.stdev(sampled_signal)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(signal):\n",
    "    features = []\n",
    "    # mean\n",
    "    features.append(signal_mean(signal[:, 0].reshape(-1))) # x\n",
    "    features.append(signal_mean(signal[:, 1].reshape(-1))) # y\n",
    "    features.append(signal_mean(signal[:, 2].reshape(-1))) # z\n",
    "    features.append(signal_mean(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    # std\n",
    "    features.append(signal_standard_deviation(signal[:, 0].reshape(-1)))\n",
    "    features.append(signal_standard_deviation(signal[:, 1].reshape(-1)))\n",
    "    features.append(signal_standard_deviation(signal[:, 2].reshape(-1)))\n",
    "    features.append(signal_standard_deviation(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    # min\n",
    "    features.append(signal_min(signal[:, 0].reshape(-1)))\n",
    "    features.append(signal_min(signal[:, 1].reshape(-1)))\n",
    "    features.append(signal_min(signal[:, 2].reshape(-1)))\n",
    "    features.append(signal_min(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    # max\n",
    "    features.append(signal_max(signal[:, 0].reshape(-1)))\n",
    "    features.append(signal_max(signal[:, 1].reshape(-1)))\n",
    "    features.append(signal_max(signal[:, 2].reshape(-1)))\n",
    "    features.append(signal_max(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    # ql\n",
    "    features.append(signal_q1(signal[:, 0].reshape(-1)))\n",
    "    features.append(signal_q1(signal[:, 1].reshape(-1)))\n",
    "    features.append(signal_q1(signal[:, 2].reshape(-1)))\n",
    "    features.append(signal_q1(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    # q2\n",
    "    features.append(signal_q2(signal[:, 0].reshape(-1)))\n",
    "    features.append(signal_q2(signal[:, 1].reshape(-1)))\n",
    "    features.append(signal_q2(signal[:, 2].reshape(-1)))\n",
    "    features.append(signal_q2(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    # q3\n",
    "    features.append(signal_q3(signal[:, 0].reshape(-1)))\n",
    "    features.append(signal_q3(signal[:, 1].reshape(-1)))\n",
    "    features.append(signal_q3(signal[:, 2].reshape(-1)))\n",
    "    features.append(signal_q3(signal[:, 3].reshape(-1))) # magnitude\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hr_features(signal):\n",
    "    features = []\n",
    "    # mean\n",
    "    features.append(signal_mean(signal)) \n",
    "\n",
    "    if ~np.isnan(signal_standard_deviation(signal)):\n",
    "        # std\n",
    "        features.append(signal_standard_deviation(signal))\n",
    "\n",
    "        # min\n",
    "        features.append(signal_min(signal))\n",
    "\n",
    "        # max\n",
    "        features.append(signal_max(signal))\n",
    "\n",
    "        # 20 percentile\n",
    "        features.append(signal_20th(signal))\n",
    "\n",
    "        # q2\n",
    "        features.append(signal_q2(signal))\n",
    "\n",
    "        # 80 percentile\n",
    "        features.append(signal_80th(signal))\n",
    "\n",
    "    else: # 만약 데이터 개수가 1개 미만이면 np.nan으로 추출\n",
    "        features.extend([np.nan]*6)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_step_features(signal):\n",
    "    features = []\n",
    "    # sum \n",
    "    features.append(signal_sum(signal)) \n",
    "    \n",
    "    if ~np.isnan(signal_standard_deviation(signal)):\n",
    "        # mean\n",
    "        features.append(signal_mean(signal)) \n",
    "\n",
    "        # std\n",
    "        features.append(signal_standard_deviation(signal))\n",
    "\n",
    "        # min\n",
    "        features.append(signal_min(signal))\n",
    "\n",
    "        # max\n",
    "        features.append(signal_max(signal))\n",
    "\n",
    "        # q1 percentile\n",
    "        features.append(signal_q1(signal))\n",
    "\n",
    "        # q2\n",
    "        features.append(signal_q2(signal))\n",
    "\n",
    "        # q3 percentile\n",
    "        features.append(signal_q3(signal))\n",
    "\n",
    "    else: # 만약 데이터 개수가 1개 미만이면 np.nan으로 추출\n",
    "        features.extend([np.nan]*7)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "seoul_tz = pytz.timezone('Asia/Seoul')\n",
    "def find_timestamp(date):\n",
    "    dt = seoul_tz.localize(datetime.fromtimestamp(date.timestamp()))\n",
    "    return dt.timestamp()*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clutering(X, range_n_clusters):\n",
    "\n",
    "    # For each number of clusters, perform Silhouette analysis and visualize the results.\n",
    "    for n_clusters in range_n_clusters:\n",
    "        \n",
    "        # Perform k-means.\n",
    "        kmeans = KMeans(n_clusters=n_clusters, \n",
    "                        init = 'k-means++',\n",
    "                        tol=1e-4,\n",
    "                        verbose=0,\n",
    "                        random_state=10)\n",
    "        y_pred = kmeans.fit_predict(X)\n",
    "        \n",
    "        \n",
    "        # Compute the Silhouette Coefficient for each sample.\n",
    "        s = metrics.silhouette_samples(X, y_pred)\n",
    "        \n",
    "        # Compute the mean Silhouette Coefficient of all data points.\n",
    "        s_mean = metrics.silhouette_score(X, y_pred)\n",
    "        \n",
    "        # For plot configuration -----------------------------------------------------------------------------------\n",
    "        fig, ax1 = plt.subplots(1, 1)\n",
    "        fig.set_size_inches(18, 7)\n",
    "        \n",
    "        # Configure plot.\n",
    "        plt.suptitle('Silhouette analysis for K-Means clustering with n_clusters: {}'.format(n_clusters),\n",
    "                    fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Configure 1st subplot.\n",
    "        ax1.set_title('Silhouette Coefficient for each sample (Mean Silhouette score: {})'.format(s_mean))\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\", fontsize=20,fontproperties=prop)\n",
    "        ax1.set_ylabel(\"Cluster\", fontsize=20,fontproperties=prop)\n",
    "        ax1.set_xlim([-1, 1])\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "        # ax1.tick_params(axis='x', labelsize='large')\n",
    "        # ax1.tick_params(axis='y', labelsize='large')\n",
    "        \n",
    "        \n",
    "        # For 1st subplot ------------------------------------------------------------------------------------------\n",
    "    \n",
    "        # Plot Silhouette Coefficient for each sample\n",
    "        cmap = cm.get_cmap(\"viridis\")\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_s = s[y_pred == i]\n",
    "            ith_s.sort()\n",
    "            size_cluster_i = ith_s.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "            color = cmap(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_s,\n",
    "                            facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "            y_lower = y_upper + 10\n",
    "            \n",
    "        # Plot the mean Silhouette Coefficient using red vertical dash line.\n",
    "        ax1.axvline(x=s_mean, color=\"red\", linestyle=\"--\",label=\"Mean Silhouette score: {}\".format(s_mean.round(3)))\n",
    "        prop_2 = FontProperties(fname=font_path,size=20)\n",
    "        ax1.legend(loc='lower left', prop=prop_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALL LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALL_LOG_SOURCE_PATH = os.path.join(envs['DATA_PATH'],\"2_preprocessed\",\"CALL_LOG\")\n",
    "CALL_LOG_DEST_PATH = os.path.join(envs['DATA_PATH'],\"3_feature_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_log_feature_enginering(df):\n",
    "    df = df.copy()\n",
    "    df['question_len'] = df['question'].apply(lambda x: len(str(x)) if pd.notna(x) else np.nan)\n",
    "    df['answer_len'] = df['answer'].apply(lambda x: len(str(x)) if pd.notna(x) else np.nan)\n",
    "    df.loc[(df['answer_len'].isnull()) & (df['question_len'] >= 0), 'answer_len'] = 0\n",
    "\n",
    "    # delete\n",
    "    # df['mute_ratio'] = df.apply(lambda row: row['mute'] / row['total_duration'] if row['total_duration'] != 0 else 0, axis=1)\n",
    "    # df['mute'] = df['mute'].apply(lambda x: 0 if x==0 else ( 1 if x >0 else x) )\n",
    "    # df = df.drop(['question','answer'], axis =1)\n",
    "    return df\n",
    "\n",
    "call_log = pd.read_csv(os.path.join(CALL_LOG_SOURCE_PATH ,\"call_log.csv\"),parse_dates=['start_second','end','date'])\n",
    "call_log = call_log_feature_enginering(call_log)\n",
    "call_log = attatch_prefix_condition(call_log,'call_log')\n",
    "\n",
    "# work activity features\n",
    "call_log = call_log[['pnum','start_second','date','call_log_total_duration','call_log_complain','call_log_mute','call_log_question_len', 'call_log_answer_len']]\n",
    "call_log.columns = ['pnum','start_second','date','(S)_call_log_duration','(S)_call_log_complaint','(NR)_call_log_mute','(S)_call_log_question_len','(R)_call_log_answer_len']\n",
    "\n",
    "# temporal features\n",
    "call_log['(S)_temporal_info_weekday'] = call_log['date'].dt.dayofweek + 1 # 월요일은 1, 일요일은 7\n",
    "call_log['hour'] = call_log['start_second'].dt.hour\n",
    "call_log['(S)_temporal_info_hour_category'] = pd.cut(call_log['hour'], bins=[0,8,10, 12, 14, 16, 18,21, np.inf ], labels=[0,1, 2, 3, 4, 5, 6,7])\n",
    "\n",
    "call_log = call_log[['pnum','start_second','date','(S)_call_log_duration','(S)_call_log_complaint','(NR)_call_log_mute','(S)_temporal_info_weekday','(S)_temporal_info_hour_category']]\n",
    "\n",
    "call_log.to_csv(os.path.join(CALL_LOG_DEST_PATH ,\"call_log.csv\"),index=False)\n",
    "\n",
    "call_log['timestamp_start'] = call_log['start_second'].apply(lambda x: find_timestamp(x))\n",
    "call_log['timestamp_end'] = call_log['end'].apply(lambda x: find_timestamp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_SOURCE_PATH = os.path.join(envs['DATA_PATH'],\"2_preprocessed\",\"ACC\")\n",
    "ACC_DEST_PATH = os.path.join(envs['DATA_PATH'],\"3_feature_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_columns=['pnum', 'start_second', 'end','data_count',\n",
    "                                                                 \n",
    "        # mean \n",
    "        '(NR)_acc_x_mean', '(NR)_acc_y_mean','(NR)_acc_z_mean','(NR)_acc_magnitude_mean',\n",
    "        # std\n",
    "        '(NR)_acc_x_std','(NR)_acc_y_std','(NR)_acc_z_std','(NR)_acc_magnitude_std',\n",
    "\n",
    "        # min\n",
    "        '(NR)_acc_x_min', '(NR)_acc_y_min', '(NR)_acc_z_min', '(NR)_acc_magnitude_min',\n",
    "\n",
    "        # max\n",
    "        '(NR)_acc_x_max', '(NR)_acc_y_max', '(NR)_acc_z_max', '(NR)_acc_magnitude_max',\n",
    "\n",
    "        # q1\n",
    "        '(NR)_acc_x_q1', '(NR)_acc_y_q1', '(NR)_acc_z_q1', '(NR)_acc_magnitude_q1',\n",
    "\n",
    "        # q2\n",
    "        '(NR)_acc_x_q2', '(NR)_acc_y_q2', '(NR)_acc_z_q2', '(NR)_acc_magnitude_q2',\n",
    "\n",
    "        # q3\n",
    "        '(NR)_acc_x_q3', '(NR)_acc_y_q3', '(NR)_acc_z_q3', '(NR)_acc_magnitude_q3'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_all = pd.read_csv(os.path.join(ACC_SOURCE_PATH,\"acc_win.csv\"),index_col=0)\n",
    "pnum_list = acc_all['pnum'].unique()\n",
    "acc_final = pd.DataFrame(columns = acc_columns)\n",
    "\n",
    "for pnum in pnum_list:\n",
    "\n",
    "    # 뽑아낼 pnum \n",
    "    pnum_call_log = call_log.query('pnum==@pnum')\n",
    "    pnum_acc = acc_all.query('pnum==@pnum')\n",
    "\n",
    "    # call log에 맞게 acc 뽑기 \n",
    "    for idx, row in pnum_call_log.iterrows():\n",
    "        start_second = row['start_second']\n",
    "        end_second = row['end']\n",
    "\n",
    "        start = row['timestamp_start']\n",
    "        end = row['timestamp_end']\n",
    "        break_ = row['break']\n",
    "        \n",
    "        filtered_call_acc = pnum_acc.query('(Timestamp >= @start) and (Timestamp <=@end)')\n",
    "\n",
    "        if not filtered_call_acc.empty:\n",
    "            call_features = get_features(filtered_call_acc[[' accX',' accY',' accZ','magnitude']].values)\n",
    "            count_call_acc = len(filtered_call_acc[[' accX',' accY',' accZ','magnitude']].notnull())\n",
    "        else:\n",
    "            call_features = [np.nan]*28\n",
    "            count_call_acc = 0\n",
    "        \n",
    "        add_row = [pnum,start_second,end_second,count_call_acc]\n",
    "        add_row.extend(call_features) \n",
    "        acc_final.loc[len(acc_final)] = add_row\n",
    "\n",
    "acc_final = acc_final[['pnum', 'start_second',\n",
    "                                                                 \n",
    "        # mean \n",
    "        '(NR)_acc_x_mean', '(NR)_acc_y_mean','(NR)_acc_z_mean','(NR)_acc_magnitude_mean',\n",
    "        # std\n",
    "        '(NR)_acc_x_std','(NR)_acc_y_std','(NR)_acc_z_std','(NR)_acc_magnitude_std',\n",
    "\n",
    "        # min\n",
    "        '(NR)_acc_x_min', '(NR)_acc_y_min', '(NR)_acc_z_min', '(NR)_acc_magnitude_min',\n",
    "\n",
    "        # max\n",
    "        '(NR)_acc_x_max', '(NR)_acc_y_max', '(NR)_acc_z_max', '(NR)_acc_magnitude_max'\n",
    "]]\n",
    "\n",
    "# Save Data\n",
    "acc_final.to_csv(os.path.join(ACC_DEST_PATH,\"acc.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_SOURCE_PATH = os.path.join(envs['DATA_PATH'],\"2_preprocessed\",\"ENV\")\n",
    "ENV_DEST_PATH = os.path.join(envs['DATA_PATH'],\"3_feature_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_columns=['pnum', 'start_second', 'end','data_count',\n",
    "                                                                \n",
    "        # mean \n",
    "        '(S)_temperature_mean', '(S)_humidity_mean','(S)_co2_mean','(S)_tvoc_mean',\n",
    "        # std\n",
    "        '(S)_temperature_std','(S)_humidity_std','(S)_co2_std','(S)_tvoc_std',\n",
    "\n",
    "        # min\n",
    "        '(S)_temperature_min', '(S)_humidity_min', '(S)_co2_min', '(S)_tvoc_min',\n",
    "\n",
    "        # max\n",
    "        '(S)_temperature_max', '(S)_humidity_max', '(S)_co2_max', '(S)_tvoc_max',\n",
    "\n",
    "        # q1\n",
    "        '(S)_temperature_q1', '(S)_humidity_q1', '(S)_co2_q1', '(S)_tvoc_q1',\n",
    "\n",
    "        # q2\n",
    "        '(S)_temperature_q2', '(S)_humidity_q2', '(S)_co2_q2', '(S)_tvoc_q2',\n",
    "\n",
    "        # q3\n",
    "        '(S)_temperature_q3', '(S)_humidity_q3', '(S)_co2_q3', '(S)_tvoc_q3',\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_all = pd.read_csv(os.path.join(ENV_SOURCE_PATH,\"env.csv\"))\n",
    "pnum_list = env_all['pnum'].unique()\n",
    "env_final = pd.DataFrame(columns = env_columns)\n",
    "\n",
    "for pnum in pnum_list:\n",
    "    # 뽑아낼 pnum\n",
    "    pnum_call_log = call_log.query('pnum==@pnum')\n",
    "    pnum_env = env_all.query('pnum==@pnum')\n",
    "\n",
    "    # call log에 맞게 env 뽑기\n",
    "    for idx, row in pnum_call_log.iterrows():\n",
    "        start_second = row['start_second']\n",
    "        end_second = row['end']\n",
    "\n",
    "        start = row['timestamp_start']\n",
    "        end = row['timestamp_end']\n",
    "        break_ = row['break']\n",
    "\n",
    "        filtered_call_env = pnum_env.query('(Timestamp >= @start) and (Timestamp <@end)')\n",
    "\n",
    "        if not filtered_call_env.empty:\n",
    "            call_features = get_features(filtered_call_env[[' Temperature', ' Humidity', ' CO2',\" TVOC\"]].values)\n",
    "            count_call_env = len(filtered_call_env[[' Temperature', ' Humidity', ' CO2',\" TVOC\"]].notnull())\n",
    "        else:\n",
    "            call_features = [np.nan]*28\n",
    "            count_call_env = 0\n",
    "        \n",
    "        add_row = [pnum,start_second,end_second, count_call_env]\n",
    "        add_row.extend(call_features)\n",
    "        env_final.loc[len(env_final)] = add_row\n",
    "\n",
    "env_final = env_final[['pnum', 'start_second',\n",
    "                                                                \n",
    "        # mean \n",
    "        '(S)_temperature_mean', '(S)_humidity_mean','(S)_co2_mean',\n",
    "        # std\n",
    "        '(S)_temperature_std','(S)_humidity_std','(S)_co2_std',\n",
    "\n",
    "        # min\n",
    "        '(S)_temperature_min', '(S)_humidity_min', '(S)_co2_min',\n",
    "\n",
    "        # max\n",
    "        '(S)_temperature_max', '(S)_humidity_max', '(S)_co2_max']]\n",
    "\n",
    "env_final.to_csv(os.path.join(ENV_DEST_PATH,\"env.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FITBIT_SOURCE_PATH = os.path.join(envs['DATA_PATH'],\"2_preprocessed\",\"FITBIT\")\n",
    "FITBIT_DEST_PATH = os.path.join(envs['DATA_PATH'],\"3_feature_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitbit_hr_all = pd.read_csv(os.path.join(FITBIT_SOURCE_PATH,'fitbit_hr.csv'),parse_dates=['ReadableTimestamp'])\n",
    "fitbit_step_all = pd.read_csv(os.path.join(FITBIT_SOURCE_PATH,'fitbit_step.csv'),parse_dates=['ReadableTimestamp'])\n",
    "\n",
    "fitbit_hr_all['Timestamp'] = fitbit_hr_all['ReadableTimestamp'].apply(lambda x: find_timestamp(x))\n",
    "fitbit_step_all['Timestamp'] = fitbit_step_all['ReadableTimestamp'].apply(lambda x: find_timestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hr_columns=['pnum', 'start_second', 'end','hr_data_count',                            \n",
    "        ## (NR)_fitbit ##                               \n",
    "        # mean \n",
    "        '(NR)_fitbit_hr_mean', \n",
    "        # std\n",
    "        '(NR)_fitbit_hr_std',\n",
    "\n",
    "        # min\n",
    "        '(NR)_fitbit_hr_min', \n",
    "\n",
    "        # max\n",
    "        '(NR)_fitbit_hr_max', \n",
    "\n",
    "        # 20\n",
    "        '(NR)_fitbit_hr_20',\n",
    "\n",
    "        # q2\n",
    "        '(NR)_fitbit_hr_q2', \n",
    "\n",
    "        # 80\n",
    "        '(NR)_fitbit_hr_q80', \n",
    "    ]\n",
    "\n",
    "\n",
    "step_columns=[ 'step_data_count'\n",
    "        # sum\n",
    "        '(NR)_fitbit_step_sum',\n",
    "\n",
    "        # mean \n",
    "        '(NR)_fitbit_step_mean',\n",
    "        \n",
    "        # std\n",
    "        '(NR)_fitbit_step_std',\n",
    "\n",
    "        # min\n",
    "        '(NR)_fitbit_step_min', \n",
    "\n",
    "        # max\n",
    "        '(NR)_fitbit_step_max',\n",
    "\n",
    "        # 20\n",
    "        '(NR)_fitbit_step_q1', \n",
    "\n",
    "        # q2\n",
    "        '(NR)_fitbit_step_q2',\n",
    "\n",
    "        # 80\n",
    "        '(NR)_fitbit_step_q3' \n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pnum_list = fitbit_hr_all['pnum'].unique()\n",
    "fitbit_final = pd.DataFrame(columns = hr_columns + step_columns)\n",
    "\n",
    "for pnum in pnum_list:\n",
    "    # 뽑아낼 pnum\n",
    "    pnum_call_log = call_log.query('pnum==@pnum')\n",
    "    pnum_fitbit_hr = fitbit_hr_all.query('pnum==@pnum')\n",
    "    pnum_fitbit_step = fitbit_step_all.query('pnum==@pnum')\n",
    "\n",
    "    # call log에 맞게 fitbit 뽑기\n",
    "    for idx, row in pnum_call_log.iterrows():\n",
    "        start_second = row['start_second']\n",
    "        end_second = row['end']\n",
    "\n",
    "        start = row['timestamp_start']\n",
    "        end = row['timestamp_end']\n",
    "        break_ = row['break']\n",
    "\n",
    "        filtered_call_fitbit_step = pnum_fitbit_step.query('(Timestamp >= @start) and (Timestamp <=@end)')\n",
    "        filtered_call_fitbit_hr = pnum_fitbit_hr.query('(Timestamp >= @start) and (Timestamp <=@end)')\n",
    "\n",
    "\n",
    "        if not filtered_call_fitbit_hr.empty:\n",
    "                hr_features = get_hr_features(filtered_call_fitbit_hr['heart-intraday_value'])\n",
    "                count_fitbit_hr = len(filtered_call_fitbit_hr[['heart-intraday_value']].notnull())\n",
    "        else:\n",
    "            hr_features = [np.nan]*7\n",
    "            count_fitbit_hr = 0\n",
    "\n",
    "\n",
    "        break_timestamp = start - break_ * 1000\n",
    "        filtered_call_fitbit_step = pnum_fitbit_step.query('(Timestamp < @start) and (Timestamp >= @break_timestamp)')\n",
    "\n",
    "        # step\n",
    "        if not filtered_call_fitbit_step.empty:\n",
    "            step_features = get_step_features(filtered_call_fitbit_step['steps-intraday_value'])\n",
    "            count_fitbit_step =len(filtered_call_fitbit_step[['steps-intraday_value']].notnull())\n",
    "        else:\n",
    "            step_features  = [np.nan]*8\n",
    "            count_fitbit_step = 0\n",
    "\n",
    "        add_row = [pnum, start_second, end_second, count_fitbit_hr]\n",
    "        add_row.extend(hr_features)\n",
    "        add_row.extend(count_fitbit_step)\n",
    "        add_row.extend(step_features)\n",
    "        fitbit_final.loc[len(fitbit_final)] = add_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitbit_final = fitbit_final[['pnum', 'start_second',                           \n",
    "        ## (NR)_fitbit ##                               \n",
    "        # mean \n",
    "        '(NR)_fitbit_hr_mean', \n",
    "        # std\n",
    "        '(NR)_fitbit_hr_std',\n",
    "\n",
    "        # min\n",
    "        '(NR)_fitbit_hr_min', \n",
    "\n",
    "        # max\n",
    "        '(NR)_fitbit_hr_max', \n",
    "\n",
    "        # 20\n",
    "        '(NR)_fitbit_hr_20',\n",
    "\n",
    "        # q2\n",
    "        '(NR)_fitbit_hr_q2', \n",
    "\n",
    "        # 80\n",
    "        '(NR)_fitbit_hr_q80' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitbit_final.to_csv(os.path.join(FITBIT_DEST_PATH,\"fitbit.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDI_DEST_PATH = os.path.join(envs['DATA_PATH'],\"3_feature_extraction\")\n",
    "\n",
    "DEMO_SOURCE_PATH = os.path.join(envs['DATA_PATH'],\"2_preprocessed\",\"INDIVID_FACTOR\")\n",
    "demo_df = pd.read_csv(os.path.join(DEMO_SOURCE_PATH,\"individual_factor.csv\"))\n",
    "\n",
    "exclude_pnum = [15,16,17]\n",
    "demo_df = demo_df.query('pnum not in @exclude_pnum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster 개수 정하는 용도\n",
    "# kmeans_clutering(individual_factor_df[['age']].values, [2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iclab/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4, \n",
    "                init = 'k-means++',\n",
    "                tol=1e-4,\n",
    "                verbose=0,\n",
    "                random_state=10)\n",
    "y_pred = kmeans.fit_predict(demo_df[['age']].values)\n",
    "demo_df['age_category'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster 개수 정하는 용도\n",
    "# kmeans_clutering(individual_factor_df[['career']].values, [2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iclab/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=4, \n",
    "                init = 'k-means+s+',\n",
    "                tol=1e-4,\n",
    "                verbose=0,\n",
    "                random_state=10)\n",
    "y_pred = kmeans.fit_predict(demo_df[['career']].values)\n",
    "demo_df['career_category'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df = demo_df[['pnum','gender','education','engage_motivation','age_category','career_category']]\n",
    "demo_df = attatch_prefix_condition(demo_df,'(IF)_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SOURCE_PATH = os.path.join(envs['DATA_PATH'],\"2_preprocessed\",\"SURVEY_DAILY\")\n",
    "daily_survey = pd.read_csv(os.path.join(DAILY_SOURCE_PATH ,'daily_before_work.csv'),parse_dates=['date'])\n",
    "daily_survey = daily_survey[['pnum','date','before_work_general_health','before_work_stress', 'before_work_arousal',\n",
    "                            'before_work_valence','self_reported_sleep_time']]\n",
    "daily_survey = attatch_prefix_condition(daily_survey,'(IF)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_factor_df= daily_survey.merge(demo_df,on=['pnum'],how='left')\n",
    "individual_factor_df.to_csv(os.path.join(INDI_DEST_PATH,\"individual_factor.csv\"),index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in_the_wild_ER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
